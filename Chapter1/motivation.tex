
The MAB model fits very well in various real-world scenarios that can be modeled as sequential decision-making problems. Some of which are mentioned as follows:-
\begin{enumerate}
\item \emph{Online Shop Domain (\cite{ghavamzadeh2015bayesian}):} In the online shop domain, a retailer aims to maximize profit by sequentially suggesting products to online shopping customers. In this scenario, at every timestep,  the retailer displays an item to a customer from a pool of items which has the highest probability of being selected by the customer. The episode ends when the customer selects or does not select a product (which will be considered as a loss to the retailer) and the process is again repeated till a pre-specified number of times with the retailer gathering valuable information regarding the customer from this behaviour and modifying its policy to display the next item.
\item \emph{Medical Treatment Design (\cite{thompson1933likelihood}):} Here at every timestep, the agent chooses to administer one out of several treatments sequentially on a patient. Here, the episode ends when the patient responds well or does not respond well to the treatment whereby the agent modifies its policy for the next suggestion.
\item \emph{Financial Portfolio Management:} In financial portfolio management MAB model can be used. Here, the agent is faced with the choice of selecting the most profitable stock option out of several stock options. The simplest strategy where we can employ a bandit model is this; at the start of every trading session the agent suggests a stock to purchase worth Re $1$, while at the closing of the trading session it sells off the stock to witness its value after a day's trading. The  profit recorded is treated as the reward revealed by the environment and the agent modifies its policy for the next day.
%\item \emph{Product Selection:} A company wants to introduce a new product in market and there is a clear separation of the test phase from the commercialization phase. In this case the company tries to minimize the loss it might incur in the commercialization phase by testing as much as possible in the test phase. So from the several variants of the product that are in the test phase the learning agent must suggest a product variant at the end of the test phase that has the highest probability of minimizing loss in the commercialization phase(see \cite{bubeck2011pure}). 
%\item \emph{Mobile Phone Channel Allocation:} Another similar problem as above concerns channel allocation for mobile phone communications (\cite{audibert2009exploration}). Here there is a clear separation between the allocation phase and communication phase whereby in the allocation phase a learning algorithm has to explore as many channels as possible to suggest the best possible channel. Each evaluation of a channel is noisy and the learning algorithm must come up with the best possible suggestion within a very small  number of attempts. 
\end{enumerate}

	The thresholding bandit problem is a special case of combinatorial MAB problem where the learner has to suggest the best set of arms above a real valued threshold. This has several relevant industrial applications. The variants of TopM problem (identifying the best $M$ arms from $K$ given arms) can be readily used in the thresholding problem.

\begin{enumerate}
\item \emph{Product Selection:} A company wants to introduce a new product in market and there is a clear separation of the test phase from the commercialization phase. In this case the company tries to minimize the loss it might incur in the commercialization phase by testing as much as possible in the test phase. So from the several variants of the product that are in the test phase the learning agent must suggest the product variant(s) that are above a particular threshold $\tau$ at the end of the test phase that have the highest probability of minimizing loss in the commercialization phase. A similar problem has been discussed for single best product variant identification without threshold in \cite{bubeck2011pure}. 
\item \emph{Mobile Phone Channel Allocation:} Another similar problem as above concerns channel allocation for mobile phone communications (\cite{audibert2009exploration}). Here there is a clear separation between the allocation phase and communication phase whereby in the allocation phase a learning algorithm has to explore as many channels as possible to suggest the best possible set of channel(s) that are above a particular threshold $\tau$. The threshold depends on the subscription level of the customer. With higher subscription the customer is allowed better channel(s) with the $\tau$ set high. Each evaluation of a channel is noisy and the learning algorithm must come up with the best possible suggestion within a very small  number of attempts.
\item \emph{Anomaly Detection and Classification:} Thresholding bandit can also be used for anomaly detection and classification where we define a cutoff level $\tau$ and for any samples above this cutoff gets classified as an anomaly. For further reading we point the reader to section 3 of \cite{locatelli2016optimal}.
\end{enumerate}


	In all the above examples the MAB model performs well mainly because all of them suffer from \textit{exploration-exploitation dilemma}. This is characterized by action-selection choice faced by the agent where it must decide whether to stay with the action yielding highest reward till now or to explore newer actions which might be more profitable in the long run. MAB's are suited for such scenarios because 
\begin{enumerate}
\item They are easy to implement.
\item The switch between exploration and exploitation is more well defined theoretically.
\item They perform well empirically.
\end{enumerate}
