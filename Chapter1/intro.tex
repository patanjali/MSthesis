One of the significant moments that has shaped human history was the invention of the computer. What essentially started out as an attempt to replace and replicate simple arithmetic functions now surrounds us on a day-to-day basis taking care of our transport, health, finance and ingratiating itself into our daily lives more and more. 

This evolution has largely been made possible due to the invention of the transistor by Bell & Schokley in 1951 which caused computers to go from the size of an airplane hangar to tiny microscopic devices.  The invention of the transistors enabled chip designers to meet the ever increasing user requirements by halving the size of the transistors and thereby doubling the number of transistors that could be packed into the same area. This phenomenon also dubbed as Mooreâ€™s law has enabled chip designers meet the exponentially increasing functionality requirements.

In addition to increasing functionality, digital devices are also expected to meet additional constraints which can be enumerated using the tuple {Power, Area and Delay}. Depending on the platform in which it gets deployed, the chip designer might have to optimize the same design to meet  two orthogonal constraints. For example, when building a server, the processor is expected to be fast while the power and area requirements are relaxed but on the other hand if the same processor is to be used on a IoT device, the designer might have to optimise the design for low power, smaller area while the device might not be expected to perform at the same speed as the server. **{ Can we rewrite it with a better example?}

We now discuss the three key objectives that a given digital design is expected to meet. 
\begin{enumerate}
    \item \textbf{Delay} With each passing generation 
\end{enumerate}





%Computers have evolved from being simple calculators to devices that can perform complex operations that are employed in various aspects of our day-to-day lives.  
%This evolution has been 

%For the last 70 years, chip designers have been able to meet this expectation by leveraging Moore's law and adding more transistors.  

%Today digital devices can be broadly classified into three different classes \begin{enumerate}
 %   \item Embedded devices: These devices are expected to interact with various aspects of a user's environment such as temperature, motion etc. and make decisions on the fly. The emergence of Internet of Things as a popular networking paradigm in the last 3 years has led to an increase in the demand for such devices. These class of devices typically application specific and perform either "send and send" or lightweight computation at the
%\end{enumerate}

%In this evolution, the expectations of the end-user from each generation has been increasing. While initial generations were expected to merely speed-up arithmetic operations, the development of commercial compute devices starting with Intel's 4004 processor has led to these devices perform complex mathematical functions at a faster rate, on a smaller device by using a little energy as possible. 












% In today's world, artificial intelligence has proved to be a game-changer in designing agents that interact with an evolving environment and make decisions on the fly. The main goal of artificial intelligence is to design artificial agents that make dynamic decisions in an evolving environment. In pursuit of these, the agent can be thought of as making a series of sequential decisions by interacting with the dynamic environment which provides it with some sort of feedback after every decision which the agent incorporates into its decision-making strategy to formulate the next decision to be made. A large number of problems in science and engineering, robotics and game playing, resource management, financial portfolio management, medical treatment design, ad placement, website optimization and packet routing can be modeled as sequential decision-making under uncertainty. Many of these real-world interesting
% sequential decision-making problems can be formulated as reinforcement learning (RL) problems (\citep{bertsekas1996neuro}, \citep{sutton1998reinforcement}). In an RL problem, an agent interacts with a dynamic, stochastic, and unknown environment, with the goal of finding an action-selection strategy or policy that optimizes some long-term performance measure. Every time when the agent interacts with the environment it receives a signal/reward from the environment based on which it modifies its policy. The agent learns to optimize the choice of actions over several time steps which is learned from the sequences of data that it receives from the environment. This is the crux of online sequential learning. 

%     This is in contrast to supervised learning methods that deal with labeled data which are independently and identically distributed (i.i.d.) samples from the considered domain and train some classifier on the entire training dataset to learn the pattern of this distribution to predict the labels of future samples (test dataset) with the assumption that it is sampled from the same domain. In contrast to this, an RL agent learns from the samples that are collected from the trajectories generated by its sequential interaction with the system. For an RL agent, the trajectory consists of a series of sequential interactions whereby it transitions from one state to another following some dynamics intrinsic to the environment while collecting the reward till some stopping condition is reached. This is known as an episode. Here, for an action $i_t$ taken by the agent at the $t$-th timestep, the agent transitions from its current state denoted by $S_{i,t}$ to state $S_{i,t+1}$ and observes the reward $X_{i,t}$. An illustrative image depicting the reinforcement learning scenario is shown in Figure \ref{fig:rl}.

% \begin{figure}[!th]
% \includegraphics[scale=0.45]{Chapter1/img/RL1.png}
% \caption{Reinforcement Learning}
% \label{fig:rl}
% \end{figure}


    
    
% %To express an RL problem more formally, we have to define the idea of Markov Decision Process (MDP) which consists of states, actions, transition probabilities, and rewards which in turn helps in deciding the strategy to be followed by the agent. 

% %An MDP consists of states
