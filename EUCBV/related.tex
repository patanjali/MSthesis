%There has been a significant amount of research in the area of stochastic MABs. One of the earliest work can be traced to \cite{thompson1933likelihood}, which deals with  the problem of choosing between two treatments to administer on patients who come in sequentially. Other seminal works include that of  \cite{robbins1952some} and then that of \cite{lai1985asymptotically} which established an asymptotic lower bound for the cumulative regret. It showed that for any consistent allocation strategy, we can have
%$\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\frac{(r^{*}-r_{i})}{D(Q_{i}||Q^{*})},$
%where $D(Q_{i}||Q^{*})$ is the Kullback-Leibler divergence between the reward densities $Q_{i}$ and $Q^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

	Bandit problems have been extensively studied in several earlier works such as \citet{thompson1933likelihood}, \citet{robbins1952some} and \citet{lai1985asymptotically}. Lai and Robbins in  \citet{lai1985asymptotically} established an asymptotic lower bound for the cumulative regret. Over the years stochastic MABs have seen several algorithms with strong regret guarantees. For further reference an interested reader can look into \citet{bubeck2012regret}. The upper confidence bound algorithms balance the exploration-exploitation dilemma by linking the uncertainty in estimate of an arm with the number of times an arm is pulled, and therefore ensuring sufficient exploration. One of the earliest among these algorithms is UCB1 \citep{auer2002finite}, which has a gap-dependent regret upper bound of  $O\left(\frac{K\log T}{\Delta}\right)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. But, the worst case gap-independent regret bound of UCB1 is found to be  $O \left(\sqrt{KT\log T}\right)$. In the later work of \citet{audibert2009minimax}, the authors propose the MOSS algorithm and showed that the worst case gap-independent regret bound of MOSS is $O\left( \sqrt{KT} \right)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$. However, the gap-dependent regret of MOSS is $O\left( \frac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$ and in certain regimes, this can be worse than even UCB1 (see \citet{audibert2009minimax,lattimore2015optimally}).
	
	 The UCB-Improved algorithm, proposed in \citet{auer2010ucb}, is a round-based\footnote{An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then eliminates one or more arms that it deems  to be sub-optimal.} variant of UCB1, that 
incurs a gap-dependent regret bound of $O\left(\frac{K\log (T\Delta^{2})}{\Delta}\right)$, which is better than that of UCB1. On the other hand, the worst case gap-independent regret bound of UCB-Improved is $O\left(\sqrt{KT\log K}\right)$. Recently in \citet{lattimore2015optimally}, the authors showed that  the algorithm OCUCB achieves order-optimal gap-dependent regret bound of $O\left(\sum_{i=2}^{K}\frac{\log\left(T/H_i\right)}{\Delta_i}\right)$ where $H_i=\sum_{j=1}^{K}\min\left\lbrace \frac{1}{\Delta_i^2},\frac{1}{\Delta_j^2}\right\rbrace$, and a gap-independent regret bound of $O\left( \sqrt{KT}\right)$. This is the best known gap-dependent and gap-independent regret bounds in the stochastic MAB framework. However, unlike our proposed EUCBV algorithm, OCUCB does not take into account the variance of the arms; as a result, empirically  we find  that our algorithm outperforms OCUCB in all the environments considered. 

	In contrast to the above work, the UCBV \citep{audibert2009exploration} algorithm utilizes variance estimates to compute the confidence intervals for each arm. UCBV has a gap-dependent regret bound of $O\left(\frac{K\sigma_{\max}^{2}\log T}{\Delta}\right)$, where $\sigma_{\max}^{2}$ denotes the maximum variance among all the arms $i\in \A$. Its gap-independent regret bound can be inferred to be same as that of UCB1 i.e $O \left(\sqrt{KT\log T}\right)$. Empirically, \citet{audibert2009exploration} showed that UCBV outperforms UCB1 in several scenarios. 
	
	Another notable design principle which has recently gained a lot of popularity is the Thompson Sampling (TS) algorithm (\citep{thompson1933likelihood}, \citep{agrawal2011analysis})  and  Bayes-UCB (BU) algorithm \citep{kaufmann2012bayesian}. % which employs the Bayesian approach in solving the MAB problem.
The TS algorithm maintains a posterior reward distribution for each arm; at each round, the algorithm samples values from these distribution and the arm corresponding to the highest sample value is chosen. Although TS is found to perform extremely well when the reward distributions are Bernoulli, it is established that with Gaussian priors the worst case regret can be as bad as $\Omega \left( \sqrt{KT\log T}\right)$ \citep{lattimore2015optimally}. The BU algorithm is an extension of the TS algorithm that takes quartile deviations into consideration while choosing arms.
	
	The final design principle we state is the information theoretic approach of DMED  \citep{honda2010asymptotically} and KLUCB \citep{garivier2011kl} algorithms. The algorithm KLUCB uses Kullbeck-Leibler divergence to compute the upper confidence bound for the arms. KLUCB is stable for a short horizon and is known to reach the \citet{lai1985asymptotically} lower bound in the special case of Bernoulli distribution. However, \citet{garivier2011kl} showed that KLUCB, MOSS and UCB1 algorithms are  empirically outperformed by UCBV in the exponential distribution as they do not take the variance of the arms into consideration. 